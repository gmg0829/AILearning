{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext as sc\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|sum(id)|\n",
      "+-------+\n",
      "|   45.0|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "# 初始化\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"FiratApp\").getOrCreate()\n",
    " \n",
    "# 下面两句都可以获取0到9的数据\n",
    "# data = spark.createDataFrame(map(lambda x: (x,), range(10)), [\"id\"])\n",
    "data = spark.range(0, 10).select(col(\"id\").cast(\"double\"))\n",
    " \n",
    "# 求和\n",
    "data.agg({'id': 'sum'}).show()\n",
    " \n",
    "# 关闭\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14156076\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "conf=SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
    "sc=SparkContext.getOrCreate(conf)\n",
    "\n",
    "#（a）利用list创建一个RDD;使用sc.parallelize可以把Python list，NumPy array或者Pandas Series,Pandas DataFrame转成Spark RDD。\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.getNumPartitions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载HDFS上面的用户数据  \n",
    "user_data = sc.textFile(\"hdfs:/input/ml-100k/u.user\")  \n",
    "#打印加载的用户信息第一条  \n",
    "user_data.first()  \n",
    "  \n",
    "#用\"|\"分割符分割每一行的数据，然后将数据返回到user_fields  \n",
    "user_fields = user_data.map(lambda line: line.split(\"|\"))  \n",
    "#统计总的用户数  \n",
    "num_users = user_fields.map(lambda fields: fields[0]).count()  \n",
    "#统计性别的种类数，distinct()函数用来去重。  \n",
    "num_genders = user_fields.map(lambda fields:fields[2]).distinct().count()  \n",
    "#统计职位种类数  \n",
    "num_occupations = user_fields.map(lambda fields:fields[3]).distinct().count()  \n",
    "#统计邮政编码种类数  \n",
    "num_zipcodes = user_fields.map(lambda fields:fields[4]).distinct().count()  \n",
    "#打印统计的这些信息  \n",
    "print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes)  \n",
    "  \n",
    "#统计用户年龄  \n",
    "ages = user_fields.map(lambda x: int(x[1])).collect()  \n",
    "#通过python中的matplotlib生成图表提供给分析师分析  \n",
    "import matplotlib.pyplo as plt  \n",
    "hist(ages, bins=20, color='lightblue', normed=True)  \n",
    "fig = plt.gcf()  \n",
    "fig.set_size_inches(16, 10)  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.csdn.net/qq_31474267/article/details/84256397\n",
    "sogou = sc.textFile(\"/home/kyle/job/sogou.txt\")\n",
    "sogou.count()\n",
    "baidu_lines = sogou.filter(lambda line: \"百度\" in line)\n",
    "baidu_lines.first()\n",
    "baidu_lines.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
